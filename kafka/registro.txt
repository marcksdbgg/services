En este registro se detalla 


Terminal 1: Zookeeper

/opt/kafka/bin/zookeeper-server-start.sh /opt/kafka/config/zookeeper.properties

Terminal 2: kafka Broker

/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties

Crear topics:

bin/kafka-topics.sh --create --topic documents.raw --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
bin/kafka-topics.sh --create --topic chunks.processed --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1
bin/kafka-topics.sh --create --topic embeddings.ready --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

Verificar su existencia:

/opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092

Para el ingest o api:

poetry run uvicorn app.main:app --host 0.0.0.0 --port 8001

Para el docproc:

poetry run python -m app.main

Politica del bucket:

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicWriteAccessForDev",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:PutObject",
            "Resource": "arn:aws:s3:::sasde/*"
        },
        {
            "Sid": "PublicGetAccessForDev",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::sasde/*"
        }
    ]
}


EJECUTAR JOB:

SLOTS=2
TM_MEM=2048m
CONNECTOR_JAR=~/jars/flink-connector-kafka-3.4.0-1.20.jar   # ajusta si lo copiaste a /lib

flink run-application \
  -t yarn-application \
  -Dyarn.application.name="FlinkRealTime" \
  -Dtaskmanager.numberOfTaskSlots=${SLOTS} \
  -Dtaskmanager.memory.process.size=${TM_MEM} \
  --parallelism ${SLOTS} \
  --jarfile ${CONNECTOR_JAR} \
  -py analytics_rt.py

Ver logs con:

yarn logs -applicationId application_1750889335808_0006 | grep "\["


txt.